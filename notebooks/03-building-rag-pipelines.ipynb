{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bacc0c-edc7-4546-b772-07a644993507",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aiembassy/workshop-rag-haystack/blob/master/notebooks/03-building-rag-pipelines.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329371d5-3d6d-40cc-b1ef-342862722eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"haystack-ai\" \\\n",
    "    \"trafilatura\" \\\n",
    "    \"qdrant-haystack\" \\\n",
    "    \"qdrant-client\" \\\n",
    "    \"huggingface-hub\" \\\n",
    "    \"sentence-transformers\" \\\n",
    "    \"nltk\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7f1fc1dbba650e",
   "metadata": {},
   "source": [
    "# Building RAG pipelines\n",
    "\n",
    "Retrieval Augmented Generation is the only reliable and efficient way to bring private data into the Large Language Models we have come up with. In this notebook, we will build a pipeline that can be used to generate text from a RAG model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c527c0fa20728d",
   "metadata": {},
   "source": "Phi-3.5 is a good choice if the resource usage is our concern, however the [warm text generation models](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending&inference=warm) list provides a lot of other models that can be used."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff2cae3a3d0dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import Secret\n",
    "from haystack.components.generators import HuggingFaceAPIGenerator\n",
    "\n",
    "generator = HuggingFaceAPIGenerator(\n",
    "    api_type=\"serverless_inference_api\",\n",
    "    api_params={\"model\": \"microsoft/Phi-3-mini-4k-instruct\"},\n",
    "    token=Secret.from_token(\"HF_TOKEN\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d28b18627caba7",
   "metadata": {},
   "source": [
    "We will use the same sentence transformer as before, but we can also experiment and swap it with another one, if we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051449a529cca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.embedders import (\n",
    "    SentenceTransformersDocumentEmbedder,\n",
    "    SentenceTransformersTextEmbedder,\n",
    ")\n",
    "\n",
    "document_embedder = SentenceTransformersDocumentEmbedder(model=\"all-MiniLM-L6-v2\")\n",
    "document_embedder.warm_up()\n",
    "\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"all-MiniLM-L6-v2\")\n",
    "text_embedder.warm_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e923989ab83c5ab3",
   "metadata": {},
   "source": [
    "Vector embeddings will be stored in Qdrant index, and we will use it to retrieve the most similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd0c483fbdde817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
    "\n",
    "document_store = QdrantDocumentStore(\n",
    "    \":memory:\",  # Never use in production systems! It's a mode only for testing purposes.\n",
    "    embedding_dim=384,  # The size of the embeddings produced by the model\n",
    "    index=\"facts\",  # We can have multiple indexes in the same database\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb8d2e79d1d048",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Haystack allows to create more complex process by building DAG-like pipelines consisting of different components. The flow of the application might be linear, or have branches choosing different paths based on the data or even by the LLM's output.\n",
    "\n",
    "First of all, it makes sense to ingest some data that we'll be playing with. Haystack provides some tools to do the web scraping, but some custom data sources are either implemented as 3rd party plugins or can be built from scratch.\n",
    "\n",
    "### Ingestion pipeline\n",
    "\n",
    "We are going to scrape some web pages and store the data in the Qdrant index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc8770b9ba64aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Python_(programming_language)\",\n",
    "    \"https://en.wikipedia.org/wiki/Java_(programming_language)\",\n",
    "    \"https://en.wikipedia.org/wiki/C_(programming_language)\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe3e443e801843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "\n",
    "link_content_fetcher = LinkContentFetcher()\n",
    "fetched_urls = link_content_fetcher.run(urls=urls)\n",
    "fetched_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2675f66f64537d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.converters import HTMLToDocument\n",
    "\n",
    "converter = HTMLToDocument()\n",
    "documents = converter.run(sources=fetched_urls[\"streams\"])\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19db304b84c8e1e",
   "metadata": {},
   "source": [
    "It generally doesn't make sense to store the whole page as a single vector. Embeddings are not lossless, and the more text we have, the more information we lose. They also have a specific context window, and the text should be split into smaller parts to get the best results.\n",
    "\n",
    "It's considered to be a good practice to split the text with some overlap between the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8f75c8a00e2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "\n",
    "document_splitter = DocumentSplitter(\n",
    "    split_by=\"sentence\", split_length=10, split_overlap=5\n",
    ")\n",
    "document_splitter.warm_up()\n",
    "\n",
    "split_documents = document_splitter.run(documents=documents[\"documents\"])\n",
    "split_documents[\"documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa3b82d5347852",
   "metadata": {},
   "source": [
    "We don't want to run all the steps manually, but rather create a pipeline that will do the job for us. It can also handle retries in case of any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca393bf138246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.writers import DocumentWriter\n",
    "\n",
    "document_writer = DocumentWriter(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636b38d367a304c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "\n",
    "ingestion_pipeline = Pipeline()\n",
    "\n",
    "# Register all the components\n",
    "ingestion_pipeline.add_component(\"link_content_fetcher\", link_content_fetcher)\n",
    "ingestion_pipeline.add_component(\"converter\", converter)\n",
    "ingestion_pipeline.add_component(\"document_splitter\", document_splitter)\n",
    "ingestion_pipeline.add_component(\"document_embedder\", document_embedder)\n",
    "ingestion_pipeline.add_component(\"document_writer\", document_writer)\n",
    "\n",
    "# Display the pipeline\n",
    "ingestion_pipeline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d6bb833227a7fd",
   "metadata": {},
   "source": [
    "It's not yet what we wanted. The components are registered, but they are not connected. Let's fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b96bb9b3cf83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the inputs and outputs are documented in the Haystack's docs\n",
    "# Example: https://docs.haystack.deepset.ai/docs/linkcontentfetcher\n",
    "ingestion_pipeline.connect(\"link_content_fetcher.streams\", \"converter.sources\")\n",
    "ingestion_pipeline.connect(\"converter.documents\", \"document_splitter.documents\")\n",
    "ingestion_pipeline.connect(\"document_splitter.documents\", \"document_embedder.documents\")\n",
    "ingestion_pipeline.connect(\"document_embedder.documents\", \"document_writer.documents\")\n",
    "\n",
    "# Now the pipeline is done and all the dots are connected\n",
    "ingestion_pipeline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287808219376e038",
   "metadata": {},
   "source": [
    "Final pipeline might be called as if it was a single component (yet, pipelines are not components in Haystack)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43aae6c6806e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion_pipeline.run(\n",
    "    data={\n",
    "        \"urls\": [\n",
    "            \"https://en.wikipedia.org/wiki/Ruby_(programming_language)\",\n",
    "            \"https://en.wikipedia.org/wiki/JavaScript\",\n",
    "            \"https://en.wikipedia.org/wiki/Rust_(programming_language)\",\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42461857e7942901",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.count_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd1f080d991226c",
   "metadata": {},
   "source": [
    "### RAG pipeline\n",
    "\n",
    "Once our data is indexed in Qdrant, we can build another pipeline that will connect to the same index and finally perform Retrieval Augmented Generation, using the LLM and the vector embeddings.\n",
    "\n",
    "For that, we need to take a text input from the user, and then use it as a query to retrieve the most similar document. The text will be used as a prompt for the LLM to generate the text, along with the fetched documents, which will be used as a context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40206e168bd28d",
   "metadata": {},
   "source": [
    "We have a document store, but also need a retriever to interact with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41945c3133eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever\n",
    "\n",
    "retriever = QdrantEmbeddingRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c70801ef57e2fe3",
   "metadata": {},
   "source": [
    "Now all the components are ready, and we can build the pipeline. However, our prompt to the LLM has to include the user's query and the documents retrieved from the index, so we need to write it first.\n",
    "\n",
    "Haystack allows to build prompts using the `PromptBuilder` class, and supports [Jinja templates](https://docs.haystack.deepset.ai/docs/jinja-templates) to allow dynamic content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf4f06cc05f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Please answer the following question using only the documents provided.\n",
    "If the question is unanswerable based solely on the documents, please\n",
    "answer \"No answer\". Do not make up any facts or details.\n",
    "\n",
    "Question: {{ text }}\n",
    "\n",
    "Documents:\n",
    "{% for doc in documents %}\n",
    "\tDocument {{ loop.index }}:\n",
    "\tDocument name: {{ doc.meta['name'] }}\n",
    "\t{{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "prompt_builder = PromptBuilder(template=prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394eb670f25efa27",
   "metadata": {},
   "source": [
    "Now it's time to assemble the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398d7cd648bf7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = Pipeline()\n",
    "\n",
    "# Register all the components\n",
    "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "rag_pipeline.add_component(\"retriever\", retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"generator\", generator)\n",
    "\n",
    "# Connect the components\n",
    "rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder.prompt\", \"generator.prompt\")\n",
    "\n",
    "# Display the pipeline\n",
    "rag_pipeline.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3af37b8824f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(input_query: str) -> str:\n",
    "    response = rag_pipeline.run(data={\"text\": input_query})\n",
    "    return response[\"generator\"][\"replies\"][0]\n",
    "\n",
    "\n",
    "print(rag(\"When was Rust programming language created?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211f3f1701c8896",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag(\"What is the most popular programming language?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4eade4a1532fb",
   "metadata": {},
   "source": [
    "## Advanced RAG techniques\n",
    "\n",
    "The basic pipeline shows how the basic RAG might be created in Haystack. However, it's a pretty new and evolving field, and there are many ways to improve it. For example, we could try to ask the LLM to first of all generate a list of steps to take to answer the question, and then call the same, or another, LLM to generate the answer. This method is called Chain-of-Thought (CoT).\n",
    "\n",
    "### Chain-of-Thought pipeline\n",
    "\n",
    "Let's rework the pipeline to include the CoT technique. We can modify the prompt template to generate the steps, and then use them to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d474a7409bf8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt_template = \"\"\"\n",
    "Please generate a list of steps to take in order to answer the following question.\n",
    "If the question is unanswerable based solely on the documents, please write down\n",
    "the list of reasons while it's impossible to answer the question. Do not produce\n",
    "anything more except the list of steps.\n",
    "\n",
    "Question: {{ text }}\n",
    "\n",
    "Documents:\n",
    "{% for doc in documents %}\n",
    "\tDocument {{ loop.index }}:\n",
    "\tDocument name: {{ doc.meta['name'] }}\n",
    "\t{{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "List of steps:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20cd1dd7224a0a4",
   "metadata": {},
   "source": [
    "Haystack does not allow to reuse the same component in different pipelines, or even twice in the same pipeline, as DAGs do not allow cycles. We have to create a new generator for the CoT pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7875e634c675b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolving_generator = HuggingFaceAPIGenerator(\n",
    "    api_type=\"serverless_inference_api\",\n",
    "    api_params={\"model\": \"microsoft/Phi-3.5-mini-instruct\"},\n",
    "    token=Secret.from_token(\"HF_TOKEN\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720fbb2db05b116a",
   "metadata": {},
   "source": [
    "It's also important to build another prompt. So, another prompt template makes a lot of sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c8cf96465f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompt_template = \"\"\"\n",
    "Please perform the list of steps provided and return a succinct answer\n",
    "to the following question. Be concise and do not provide any additional\n",
    "information except the answer.\n",
    "\n",
    "Question: {{ text }}\n",
    "\n",
    "Steps: {{ replies[0] }}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "cot_prompt_builder = PromptBuilder(\n",
    "    template=cot_prompt_template, required_variables=[\"replies\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bdf619c30c4ca9",
   "metadata": {},
   "source": [
    "Extending the pipeline with the CoT technique is as simple as adding a few more components and connecting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f32fb6b811bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline.add_component(\"cot_prompt_builder\", cot_prompt_builder)\n",
    "rag_pipeline.add_component(\"resolving_generator\", resolving_generator)\n",
    "\n",
    "rag_pipeline.connect(\"generator.replies\", \"cot_prompt_builder.replies\")\n",
    "rag_pipeline.connect(\"cot_prompt_builder.prompt\", \"resolving_generator.prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a914d3e42095195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cot_rag(input_query: str) -> str:\n",
    "    response = rag_pipeline.run(\n",
    "        data={\n",
    "            \"text_embedder\": {\"text\": input_query},\n",
    "            \"prompt_builder\": {\"text\": input_query, \"template\": new_prompt_template},\n",
    "            \"cot_prompt_builder\": {\"text\": input_query},\n",
    "        }\n",
    "    )\n",
    "    return response[\"resolving_generator\"][\"replies\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca412bc164070db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cot_rag(\"When was Rust programming language created?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eea721ab4d26f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cot_rag(\"What is the most popular programming language?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ad128d34a49a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
