{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380491ec-df08-484e-98c0-7ef563845b91",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aiembassy/workshop-rag-haystack/blob/master/notebooks/01-using-llms.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d98ff32-a211-4050-bdc1-8c5e3ed29f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"haystack-ai\" \\\n",
    "    \"huggingface-hub\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af858c335aefacef",
   "metadata": {},
   "source": [
    "# Using LLMs in Haystack\n",
    "\n",
    "One of the benefits of using frameworks like Haystack for all the interactions with LLMs is that it provides a unified interface for different models. This way, you can switch between models without changing the code\n",
    "too much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce5d6aa33b1ee9",
   "metadata": {},
   "source": [
    "Selecting the right LLM for a task is a challenge per se. Obviously, OpenAI or Anthropic models are easy to start with, but building a fully private LLM can't be done with commercial models. HuggingFace provides a hub of permissive models that can be used for free, and their Serverless Inference API is a good way to try them out.\n",
    "\n",
    "**Hint:** please have a look at the list of the [warm text generation models](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending&inference=warm), so you don't have to wait to spin up a new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c188c7059cc883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import Secret\n",
    "from haystack.components.generators import HuggingFaceAPIGenerator\n",
    "\n",
    "generator = HuggingFaceAPIGenerator(\n",
    "    api_type=\"serverless_inference_api\",\n",
    "    api_params={\"model\": \"microsoft/Phi-3.5-mini-instruct\"},\n",
    "    token=Secret.from_token(\"HF_TOKEN\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eadd5382796f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generator.run(prompt=\"Which continent has the most countries?\")\n",
    "print(response[\"replies\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abb6f5342a8b87f",
   "metadata": {},
   "source": [
    "## Local models\n",
    "\n",
    "Although it's possible to run HuggingFace models inside the application, by using `HuggingFaceLocalGenerator`, it's not recommended for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e63b27db09313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.generators import HuggingFaceLocalGenerator\n",
    "\n",
    "local_generator = HuggingFaceLocalGenerator(\n",
    "    model=\"distilbert/distilgpt2\",  # The smallest generative model available on HF\n",
    "    token=Secret.from_token(\"HF_TOKEN\"),\n",
    ")\n",
    "local_generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351cd0ae51dc31db",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = local_generator.run(prompt=\"Which continent has the most countries?\")\n",
    "print(response[\"replies\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330fc2bbc041c8fb",
   "metadata": {},
   "source": [
    "There are other tools intended to deploy LLMs, like Ollama or Llama.cpp, and both might be used in Haystack applications.\n",
    "\n",
    "- [`OllamaGenerator`](https://docs.haystack.deepset.ai/docs/ollamagenerator)\n",
    "- [`LlamaCppGenerator`](https://docs.haystack.deepset.ai/docs/llamacppgenerator)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
